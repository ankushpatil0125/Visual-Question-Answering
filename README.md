# Visual Question Answering (VQA) Project

## Overview
This project aims to develop a Visual Question Answering (VQA) model using state-of-the-art deep learning architectures, focusing on integrating Vision Transformer (ViT), BERT, and Dinov2 models. The goal is to create a system that can answer questions about images in natural language.

## Features
- **Model Architectures**: Utilizes Vision Transformer (ViT) for image embeddings and BERT for text embeddings.
- **Low-Rank Adaptation (LoRA)**: Implements LoRA to optimize and fine-tune pre-trained models for improved performance and efficiency.
- **Training and Evaluation**: Experimented with different architectures including BERT + Dinov2 with cross-attention, achieving optimal results.
- **Technologies**: Implemented using Python, PyTorch, TensorFlow, and Hugging Face's Transformers library.
- **Dataset**: Used the VQA dataset from Georgia Tech, focusing on the Balanced Real Images subset.
